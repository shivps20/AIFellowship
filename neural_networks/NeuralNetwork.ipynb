{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f7f38a-bcd6-4638-9607-eb63d0e51fc0",
   "metadata": {},
   "source": [
    "# Neural Network Introduction\n",
    "\n",
    "A neural network is a computational model inspired by the way human brains work. It is designed to recognize patterns and learn from data. The network learns to map input data (like pixels in an image) to an output (like a label/target \"cat\" or \"dog\") by adjusting internal parameters.\n",
    "\n",
    "### Real-Life Analogy\n",
    "Think of a neural network like a child learning to differentiate between apples and oranges. The more fruits the child sees, the better they get at recognizing the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5be85c-1110-4e06-84ca-667d1a24410e",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "1. **Neuron** - Basic units that receive inputs, process them, and produce outputs. Each Neuron holds a numbre between 0 and 1\n",
    "2. **Input Layers** - Receives initial data.\n",
    "3. **Hidden Layer** - Perform computations\n",
    "4. **Output Layer** - Produces the final prediction.\n",
    "5. **Weights**: Numbers that determine the strength or importance of input features. ***Number of weights = Number of Hidden Layer + 1*** (1 for output layer)\n",
    "6. **Biases**: Constants added to the weighted input to shift the activation function.\n",
    "7. **Activation Function**: A non-linear function that decides whether the neuron should be activated.\n",
    "\n",
    "### Example\n",
    "**Predicting House Prices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb58f1-a4d6-46fa-87c1-fcb6ac98e001",
   "metadata": {},
   "source": [
    "## Forward Propagation (Pass)\n",
    "Forward propagation is how data flows through the network to make a prediction.\n",
    "\n",
    "### Steps\n",
    "1. Multiply input by weight.\n",
    "2. Add bias.\n",
    "3. Apply activation function.\n",
    "4. Pass result to next layer.\n",
    "\n",
    "### Formula\n",
    "***Output = Activation(Weight * Input + Bias)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7413f-29c0-420f-bef4-910470f61469",
   "metadata": {},
   "source": [
    "## Training the Network - (Backpropagation + Gradient Descent)\n",
    "\n",
    "Training involves - Analyzing the output from the **Forward Pass**, calculating the error, then ***adjusting weights and biases to reduce error***. This will be an iterative cycle.\n",
    "\n",
    "### Steps\n",
    "1. **Forward pass**: Predict output.\n",
    "2. **Loss function**: Measure how wrong the prediction is.\n",
    "3. **Backward pass (Backpropagation)**: Calculate how much each weight contributed to the error.\n",
    "4. **Gradient Descent**: Update weights and biases in the opposite direction of error gradient.\n",
    "\n",
    "### Analogy - Shooting Game\n",
    "\n",
    "1. Forward Pass: You aim and shoot at the target\n",
    "2. Loss Function: Calculate by how much you missed the target (assuming target is missed)\n",
    "3. Backward Pass: Calculate the angle to be adjusted to hit the target\n",
    "4. Gradient Descent: Update anlges (Weights & Biases) in the opposite direction of the error\n",
    "5. Repeat the cycle from Step-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6bfd14-65df-4513-8065-1e3e8de73357",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "An activation function ***introduces non-linearity into the output of a neuron***. Without it, no matter how many layers you stack, the entire neural network would behave like a linear function. That means it would not be able to learn complex patterns such as images, speech, XOR logic, etc.\n",
    "\n",
    "#### Why is Non-Linearity Important?\n",
    "Linear equations can only solve simple problems like drawing a straight line to separate data. But most real-world problems are non-linear, such as:\n",
    "\n",
    "1. Classifying handwritten digits\n",
    "2. Detecting emotions from voice\n",
    "3. Predicting stock market trends\n",
    "\n",
    "Activation functions help the network bend the decision boundary to fit such complex patterns.\n",
    "\n",
    "#### Example:\n",
    "Imagine you're building a spam classifier: <br>\n",
    "If you don't use an activation function: All emails are classified as either spam or not based on a linear rule → likely inaccurate.<br>\n",
    "With activation functions, the network can learn complex rules like:\n",
    "\n",
    "\"If the word 'FREE' appears multiple times and it's from an unknown sender and has a link, then it's spam.\"\n",
    "\n",
    "**Common Functions for Activation:**\n",
    "1. ***ReLU (Rectified Linear Unit)***: f(x) = max(0, x)\n",
    "2. ***Sigmoid***: f(x) = 1 / (1 + exp(-x)) → used for probabilities\n",
    "3. ***Tanh***: Outputs between -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4c68a-06c5-4876-be69-380f7362ebcf",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "To quantify how far off the network's prediction is from the actual value.\n",
    "\n",
    "### Common Loss Functions\n",
    "1. ***Mean Squared Error (MSE)*** for regression\n",
    "2. **Cross Entropy*** for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6930dd9-72f1-4d71-a95c-3792bbbb221a",
   "metadata": {},
   "source": [
    "## Sample Code Example\n",
    "**Input**  - Input array where each row has two values (0 or 1) <br>\n",
    "**Output** - XOR Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9445512-b2ea-4760-899c-5359597dbf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define activation and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ab868ca-9367-4f97-841e-3fd33a44b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Values...........................\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "\n",
      "\n",
      "Output Values...........................\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Input and Output Data\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])         # Inputs\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])             # XOR Output (to see learning capability)\n",
    "\n",
    "print(\"Input Values...........................\")\n",
    "print(X)\n",
    "print(\"\\n\\nOutput Values...........................\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad9a4d7-4cbc-45c0-a136-b13dd3a2e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize Weights and Biases\n",
    "np.random.seed(1)\n",
    "input_layer_size = 2  # 2 Features\n",
    "hidden_layer_size = 4 # 4 neurons in the hidden layer\n",
    "output_layer_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "909c6654-41ee-4a41-8e38-c718556d5271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1, W2, initialized randomly\n",
    "\n",
    "# Weights - initializes the weights between the input layer and the hidden layer of your neural network using random values\n",
    "W1 = np.random.rand(input_layer_size, hidden_layer_size)     # (2x4) Creates a matrix of shape (2, 4) filled with random numbers between 0 and 1.\n",
    "W2 = np.random.rand(hidden_layer_size, output_layer_size)    # (4x1) Creates a matrix of shape (4, 1) filled with random numbers between 0 and 1.\n",
    "\n",
    "# W1 - Weight Matrix : Each element in this matrix is a weight that determines how strongly a specific input neuron affects a specific hidden neuron.\n",
    "# W2 - Weight Matrix : Each element in this matrix is a weight that determines how strongly a specific hidden neuron affects a specific output neuron.\n",
    "\n",
    "# W2 - weights from hidden layer to output layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257fab26-9ee6-42f7-a742-4e6b70c55eb2",
   "metadata": {},
   "source": [
    "### Why Random Initialization?\n",
    "If all weights started as zeros: Every neuron would learn the same thing.\n",
    "The network wouldn't be able to break symmetry or learn meaningful patterns.\n",
    "Random values help each neuron begin with slightly different behavior, allowing the network to learn a rich variety of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800bad43-4cbb-4ad2-8a31-049ef9a37d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biases\n",
    "B1 = np.zeros((1, hidden_layer_size))  # (1x4) - Hidden layer bias vector. creats a matrix of size 1x4\n",
    "B2 = np.zeros((1, output_layer_size))  # (1x1) - Output layer bias vector. creats a matrix of size 1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac9b119-dff1-44de-8bc9-b4ce1ae31f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 0.0059\n",
      "Epoch 1000 - Loss: 0.0043\n",
      "Epoch 2000 - Loss: 0.0034\n",
      "Epoch 3000 - Loss: 0.0028\n",
      "Epoch 4000 - Loss: 0.0023\n",
      "Epoch 5000 - Loss: 0.0020\n",
      "Epoch 6000 - Loss: 0.0018\n",
      "Epoch 7000 - Loss: 0.0016\n",
      "Epoch 8000 - Loss: 0.0014\n",
      "Epoch 9000 - Loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train the network\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass - Using sigmoid activation\n",
    "    Z1 = np.dot(X, W1) + B1   # Linear combination at hidden layer\n",
    "    A1 = sigmoid(Z1)          # Activation from hidden layer\n",
    "    Z2 = np.dot(A1, W2) + B2  # Linear combination at output layer\n",
    "    A2 = sigmoid(Z2)          # Final output activation\n",
    "\n",
    "    # Loss (Mean Squared Error)\n",
    "    loss = np.mean((y - A2) ** 2)\n",
    "\n",
    "    # Backward pass - Chain rule and gradient computation\n",
    "    dA2 = (A2 - y)\n",
    "    dZ2 = dA2 * sigmoid_derivative(A2)\n",
    "    dW2 = np.dot(A1.T, dZ2)\n",
    "    dB2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * sigmoid_derivative(A1)\n",
    "    dW1 = np.dot(X.T, dZ1)\n",
    "    dB1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update weights and biases - Using learning_rate * gradients\n",
    "    W1 -= learning_rate * dW1\n",
    "    B1 -= learning_rate * dB1\n",
    "    W2 -= learning_rate * dW2\n",
    "    B2 -= learning_rate * dB2\n",
    "\n",
    "    # Print loss every 1000 iterations\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch} - Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbd64acc-85a3-4e0c-962f-094eb2fb957f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Output after Training:\n",
      "[[0.04]\n",
      " [0.97]\n",
      " [0.97]\n",
      " [0.03]]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Final Prediction\n",
    "print(\"\\nFinal Output after Training:\")\n",
    "print(np.round(A2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070cc49-d473-4378-9ed0-f1e39b45b146",
   "metadata": {},
   "source": [
    "### Final Output Interpretation (XOR)\n",
    "The values are probabilities from the sigmoid function. Predictions > 0.5 are class 1, < 0.5 are class 0.\n",
    "\n",
    "| Input   | Expected | Predicted               |\n",
    "| ------- | -------- | ----------------------- |\n",
    "| \\[0, 0] | 0        | **0.04** (close to 0 ✅) |\n",
    "| \\[0, 1] | 1        | **0.97** (close to 1 ✅) |\n",
    "| \\[1, 0] | 1        | **0.97** (close to 1 ✅) |\n",
    "| \\[1, 1] | 0        | **0.03** (close to 0 ✅) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
